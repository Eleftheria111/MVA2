{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSu3U-SNu8v6",
        "outputId": "36cde625-f8f7-4037-b699-67a041775455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /opt/miniconda3/lib/python3.13/site-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.11.12)\n",
            "Requirement already satisfied: opencv-python in /opt/miniconda3/lib/python3.13/site-packages (4.12.0.88)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.13/site-packages (2.2.6)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/lib/python3.13/site-packages (0.23.0)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# ===== INSTALL DEPENDENCIES =====\n",
        "!pip install huggingface_hub\n",
        "!pip install boto3 -q\n",
        "!pip install opencv-python torch numpy torchvision tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Lo27xcqrOMq"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from huggingface_hub import hf_hub_download\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMr99Yo7x8N1"
      },
      "source": [
        "# Please double, triple, quadruple check that the below code runs without errors before submitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YhoE1nF2Pee"
      },
      "source": [
        "## TODO 1 - Enter your HuggingFace username below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ENyfncieqs6i"
      },
      "outputs": [],
      "source": [
        "hf_username = \"EleftheriaK\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lC0jUquq_06"
      },
      "source": [
        "## TODO 2 - Define your model EXACTLY as you did in your training code (otherwise there will be errors, and, possibly, tears).\n",
        "\n",
        "Note below the classname is 'YourModelArchitecture'. That's because it literally needs to be YOUR MODEL ARCHITECTURE. This class definition is later referred to below in the 'load_model_from_hub' method. The architecture must match here, or it will not be able to instantiate the model weights correctly once it downloads them from HuggingFace. Pay very close attention to getting this right, please.\n",
        "\n",
        "Replace the below code, and replace the corresponding line in the 'load_model_from_hub' method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cm-y1pPnOGkK"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1. MODEL DEFINITION (must match training)\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "class YourModelArchitecture(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super().__init__()\n",
        "        weights = R2Plus1D_18_Weights.DEFAULT\n",
        "        self.backbone = r2plus1d_18(weights=weights)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #(B, C, T, H, W)\n",
        "        return self.backbone(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qahq0xG2rs4h"
      },
      "source": [
        "## Download the test data from s3, and create the corresponding dataset + dataloader.\n",
        "\n",
        "There's no TODO for you here. This text is just here to explain to you what this code does.\n",
        "\n",
        "In this instance, the test data IS the training data you were provided in the Model Training notebook. This is by design. You do not have access to the test data. This is a simple check to make sure the mechanics of this notebook work.\n",
        "\n",
        "You should achieve the same accuracy here in this notebook, as you did in your previous notebook (random seed notwithstanding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XBukVn9qrnFZ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DOWNLOAD TEST DATA FROM S3\n",
        "# =============================================================================\n",
        "\n",
        "def download_test_data(bucket_name='training-and-validation-data',download_dir='./test-data'):\n",
        "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "    bucket_name = 'prism-mvta'\n",
        "    prefix = 'training-and-validation-data/'\n",
        "\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "    video_names = []\n",
        "\n",
        "    for page in pages:\n",
        "        if 'Contents' not in page:\n",
        "            print(\"No files found at the specified path!\")\n",
        "            break\n",
        "\n",
        "        print(\"Downloading test data:\\n\")\n",
        "        for obj in tqdm(page['Contents']):\n",
        "            key = obj['Key']\n",
        "            filename = os.path.basename(key)\n",
        "\n",
        "            if not filename:\n",
        "                continue\n",
        "\n",
        "            video_names.append(filename)\n",
        "            local_path = os.path.join(download_dir, filename)\n",
        "            # print(f\"Downloading: {filename}\")\n",
        "            s3.download_file(bucket_name, key, local_path)\n",
        "\n",
        "    print(f\"\\nDownloaded {len(video_names)} test videos\")\n",
        "    return download_dir\n",
        "\n",
        "\n",
        "# ============================================================================= # DATASET AND DATALOADER =============================================================================\n",
        "weights = R2Plus1D_18_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"Dataset for loading videos from a folder. Labels from filename prefix.\"\"\"\n",
        "\n",
        "    def __init__(self, video_dir, frame_size=(112, 112), target_frames=16, augment=None, do_preprocess=True):\n",
        "        self.video_dir = video_dir\n",
        "        self.frame_size = frame_size\n",
        "        self.target_frames = target_frames\n",
        "        self.augment = augment\n",
        "        self.do_preprocess = do_preprocess\n",
        "\n",
        "        self.video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "        self.labels = [int(f.split('_')[0]) - 1 for f in self.video_files] \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
        "        frames = self._load_video(video_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            frames = self.augment(frames)\n",
        "        \n",
        "        if self.do_preprocess:\n",
        "           x = frames.permute(1,0,2,3)\n",
        "           x = preprocess(x)\n",
        "           frames = x.permute(1,0,2,3)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def _load_video(self, path, target_frames=16):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        all_frames = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            all_frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        T = self.target_frames\n",
        "        H, W = self.frame_size\n",
        "\n",
        "        if len(all_frames) == 0:\n",
        "            return torch.zeros(3, T, H, W)\n",
        "\n",
        "        idxs = np.linspace(0, len(all_frames) - 1, T).astype(int)\n",
        "        sampled = [cv2.resize(all_frames[i], (W, H)) for i in idxs]\n",
        "\n",
        "        frames = torch.from_numpy(np.array(sampled)).permute(3, 0, 1, 2).float() / 255.0\n",
        "\n",
        "        return frames\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    frames_list, labels = zip(*batch)\n",
        "    frames = torch.stack(frames_list)\n",
        "    frames = frames.permute(0, 2, 1, 3, 4)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return frames, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9PVSdWKsP94"
      },
      "source": [
        "## TODO 3 - Download your model from HuggingFace and instantiate it\n",
        "\n",
        "Replace line 8 of the below code. Line 8 is where you instantiate YOUR MODEL ARCHITECTURE (which you re-defined above) with the weights you download from HuggingFace. Make sure you get the class name, and the arguments to the __init__ method correct.\n",
        "\n",
        "\n",
        "This code just downloads the same model which you uploaded in the last notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "203abeec725944909cb8ec2b39f3e2af",
            "70a4207be31e43ad811c8abd3be7f9ad",
            "2a4dd8eb46d5479abdea9d214c6e541c",
            "1c3c510ca9a248a1b942e4b5e3767cf2",
            "6747f1470898402b81ef8c0a3eb94245",
            "cd65765c817a4a35b79cb0d58a4f3a86",
            "b431d05102dc4ea28feecc8c4dc13916",
            "c5777468dc0f413e9237218bf5dc9d21",
            "25baeea261454e7a819d848e8940f350",
            "db6d5a814c6c45ba86470919d259619d",
            "10234769f7ed4e73b3348a9bd5d9e2ed"
          ]
        },
        "id": "LWuMOqY_sOdg",
        "outputId": "f65c7fe1-4950-4229-a51d-87d4b9e43fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from EleftheriaK/mv-final-assignment\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DOWNLOAD MODEL FROM HUGGING FACE\n",
        "# =============================================================================\n",
        "\n",
        "def load_model_from_hub(repo_id, num_classes=7):\n",
        "    model_path = hf_hub_download(repo_id=repo_id, filename=\"model.pt\")  # see note below if filename differs\n",
        "\n",
        "    model = YourModelArchitecture(num_classes=num_classes)\n",
        "\n",
        "    sd = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "    # If weights don't have \"backbone.\" but model expects it, add it\n",
        "    if not any(k.startswith(\"backbone.\") for k in sd.keys()):\n",
        "        sd = {f\"backbone.{k}\": v for k, v in sd.items()}\n",
        "\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    print(f\"Model loaded from {repo_id}\")\n",
        "    return model\n",
        "\n",
        "model = load_model_from_hub(f\"{hf_username}/mv-final-assignment\",num_classes=7)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NycfLBRksum4"
      },
      "source": [
        "## TODO 4\n",
        "\n",
        "Make sure the below code correctly evaluates your model performance on the given data!\n",
        "\n",
        "This is your last chance to verify this before submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzgdieGiw4_k",
        "outputId": "f655601a-a58c-449f-c5d4-3a3de3473f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Downloading test data:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [05:36<00:00,  4.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloaded 77 test videos\n",
            "\n",
            "Running inference on 77 test videos...\n",
            "\n",
            "\n",
            "✓  pred=1  true=1  |    742.5ms  |  2_sadfasjldkfjaseifj.mp4\n",
            "✓  pred=1  true=1  |      4.7ms  |  2_sdafkjaslkclaksdjkas.mp4\n",
            "✓  pred=3  true=3  |      4.6ms  |  4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "✓  pred=2  true=2  |      4.1ms  |  3_kling_dskfseu.mp4\n",
            "✓  pred=3  true=3  |      4.0ms  |  4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_kling_kdjflaskdjf.mp4\n",
            "✓  pred=1  true=1  |      4.0ms  |  2_dsalkfjalwkenlke.mp4\n",
            "✓  pred=2  true=2  |      4.2ms  |  3_dsjlaeijlksjdfie.mp4\n",
            "✓  pred=2  true=2  |      4.1ms  |  3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "✓  pred=3  true=3  |      4.2ms  |  4_sadlfkjlknewkjejk.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "✓  pred=1  true=1  |      4.3ms  |  2_difficult_2.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_sdlkjslndflkseijlkjef.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "✓  pred=5  true=5  |      3.7ms  |  6_dfjewaijsldkjfsaef.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "✓  pred=1  true=1  |      3.9ms  |  2_dkdjwkndkfw.mp4\n",
            "✓  pred=2  true=2  |      4.2ms  |  3_sadklfjasbnlkjlfkj.mp4\n",
            "✓  pred=1  true=1  |      4.1ms  |  2_dkjd823kjf.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "✓  pred=1  true=1  |      3.7ms  |  2_sdkjdsflkjfwa.mp4\n",
            "✓  pred=2  true=2  |      3.7ms  |  3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "✓  pred=3  true=3  |      3.8ms  |  4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "✗  pred=2  true=4  |      3.8ms  |  5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "✓  pred=1  true=1  |      3.8ms  |  2_sdfkjsaleijflaskdjf.mp4\n",
            "✓  pred=1  true=1  |      3.9ms  |  2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "✓  pred=2  true=2  |      4.2ms  |  3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "✓  pred=2  true=2  |      4.1ms  |  3_sadlfkjawelnflksdjf.mp4\n",
            "✓  pred=3  true=3  |      3.8ms  |  4_sadflkjasldkjfalseij.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_asdlkfjalsflnekj.mp4\n",
            "✓  pred=1  true=1  |      3.8ms  |  2_dkdmkejkeimdh.mp4\n",
            "✓  pred=4  true=4  |      4.1ms  |  5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "✓  pred=1  true=1  |      3.8ms  |  2_dfsaeklnvvalkej.mp4\n",
            "✓  pred=3  true=3  |      4.2ms  |  4_aslkcasckmwlejk.mp4\n",
            "✓  pred=3  true=3  |      4.1ms  |  4_aslkjasmcalkewjlkje.mp4\n",
            "✓  pred=1  true=1  |      3.9ms  |  2_sdjfhafsldkjhjk.mp4\n",
            "✗  pred=1  true=2  |      3.9ms  |  3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "✓  pred=2  true=2  |      3.8ms  |  3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_sdlkfjalkjejafe.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_sdlkjfaslkjfalskjdf.mp4\n",
            "✓  pred=2  true=2  |      3.8ms  |  3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "✓  pred=3  true=3  |      3.8ms  |  4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "✓  pred=3  true=3  |      4.2ms  |  4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "✗  pred=2  true=3  |      3.9ms  |  4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "✗  pred=1  true=2  |      3.9ms  |  3_dsksdfjbvsdkj.mp4\n",
            "✓  pred=1  true=1  |      4.0ms  |  2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "✓  pred=2  true=2  |      3.7ms  |  3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "✗  pred=3  true=2  |      4.2ms  |  3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "✓  pred=3  true=3  |      4.1ms  |  4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "✓  pred=2  true=2  |      3.8ms  |  3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "✓  pred=2  true=2  |      3.9ms  |  3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "✓  pred=2  true=2  |      4.2ms  |  3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "✓  pred=5  true=5  |      3.9ms  |  6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_sdfjwaiejflkasjdf.mp4\n",
            "✓  pred=1  true=1  |      3.8ms  |  2_sdlfjlewlkjkj.mp4\n",
            "✓  pred=1  true=1  |      4.0ms  |  2_difficult_sdafkljsalkfj.mp4\n",
            "✓  pred=3  true=3  |      3.9ms  |  4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "✓  pred=2  true=2  |      3.8ms  |  3_dslkaldskjflakjs.mp4\n",
            "✗  pred=2  true=3  |      3.7ms  |  4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "✓  pred=2  true=2  |      4.0ms  |  3_sdlkfjaleknaksej.mp4\n",
            "✓  pred=2  true=2  |      4.1ms  |  3_asldkfjalwieaskdfaskdf.mp4\n",
            "✓  pred=0  true=0  |      3.8ms  |  1_dksksjfwijf.mp4\n",
            "✓  pred=2  true=2  |      3.8ms  |  3_sdflkjliejkjdf.mp4\n",
            "✓  pred=3  true=3  |      4.0ms  |  4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "✗  pred=5  true=3  |      3.9ms  |  4_dssalsdkfjweijf.mp4\n",
            "✗  pred=5  true=6  |      3.9ms  |  7_sadkjfkljekj.mp4\n",
            "✓  pred=1  true=1  |      3.8ms  |  2_sdlkjsaelijfksdjf.mp4\n",
            "✓  pred=2  true=2  |      3.7ms  |  3_dkk873lkjlksajdf.mp4\n",
            "✓  pred=3  true=3  |      4.0ms  |  4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "\n",
            "==================================================\n",
            "SUMMARY\n",
            "==================================================\n",
            "Total videos:         77\n",
            "Correct:              69\n",
            "Incorrect:                8\n",
            "\n",
            "ACCURACY:             89.61%\n",
            "\n",
            "Total time:           41.55s\n",
            "Avg per video:        13.5ms\n",
            "Min latency:          3.7ms\n",
            "Max latency:          742.5ms\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, test_loader, dataset, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_times = []\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (frames, labels) in enumerate(test_loader):\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "            # Time the forward pass\n",
        "            start_time = time.time()\n",
        "            outputs = model(frames)\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()  # wait for GPU to finish\n",
        "            end_time = time.time()\n",
        "\n",
        "            inference_time = (end_time - start_time) * 1000  # ms\n",
        "            all_times.append(inference_time)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            for i in range(labels.size(0)):\n",
        "                batch_idx = idx * test_loader.batch_size + i\n",
        "                video_name = dataset.video_files[batch_idx]\n",
        "                pred = preds[i].item()\n",
        "                true_label = labels[i].item()\n",
        "                is_correct = \"✓\" if pred == true_label else \"✗\"\n",
        "\n",
        "                print(f\"{is_correct}  pred={pred}  true={true_label}  |  {inference_time:>7.1f}ms  |  {video_name}\")\n",
        "\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_preds, all_labels, all_times\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN INFERENCE\n",
        "# =============================================================================\n",
        "\n",
        "def run_inference(model, bucket_name='training-and-validation-data'):\n",
        "    device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\")\n",
        "    \n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Download test data\n",
        "    test_dir = download_test_data(bucket_name, './test-data')\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create dataloader\n",
        "    test_dataset = VideoDataset(test_dir, frame_size=(224, 224))\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"\\nRunning inference on {len(test_dataset)} test videos...\")\n",
        "\n",
        "    # Warmup (optional, helps get consistent GPU timings)\n",
        "    if device.type == 'cuda':\n",
        "        dummy = torch.randn(1, 3, 1000, 224, 224).to(device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    total_start = time.time()\n",
        "    accuracy, preds, labels, times = evaluate(model, test_loader, test_dataset, device)\n",
        "    total_end = time.time()\n",
        "\n",
        "    # Summary\n",
        "    num_correct = sum(p == l for p, l in zip(preds, labels))\n",
        "    num_wrong = len(preds) - num_correct\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total videos:         {len(preds)}\")\n",
        "    print(f\"Correct:              {num_correct}\")\n",
        "    print(f\"Incorrect:                {num_wrong}\")\n",
        "    print(f\"\")\n",
        "    print(f\"ACCURACY:             {accuracy*100:.2f}%\")\n",
        "    print(f\"\")\n",
        "    print(f\"Total time:           {total_end - total_start:.2f}s\")\n",
        "    print(f\"Avg per video:        {sum(times) / len(times):.1f}ms\")\n",
        "    print(f\"Min latency:          {min(times):.1f}ms\")\n",
        "    print(f\"Max latency:          {max(times):.1f}ms\")\n",
        "    print(\"=\"*50)\n",
        "    return accuracy, preds, labels\n",
        "\n",
        "_, _, _ = run_inference(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10234769f7ed4e73b3348a9bd5d9e2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c3c510ca9a248a1b942e4b5e3767cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db6d5a814c6c45ba86470919d259619d",
            "placeholder": "​",
            "style": "IPY_MODEL_10234769f7ed4e73b3348a9bd5d9e2ed",
            "value": " 611k/611k [00:01&lt;00:00, 592kB/s]"
          }
        },
        "203abeec725944909cb8ec2b39f3e2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70a4207be31e43ad811c8abd3be7f9ad",
              "IPY_MODEL_2a4dd8eb46d5479abdea9d214c6e541c",
              "IPY_MODEL_1c3c510ca9a248a1b942e4b5e3767cf2"
            ],
            "layout": "IPY_MODEL_6747f1470898402b81ef8c0a3eb94245"
          }
        },
        "25baeea261454e7a819d848e8940f350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a4dd8eb46d5479abdea9d214c6e541c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5777468dc0f413e9237218bf5dc9d21",
            "max": 611445,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25baeea261454e7a819d848e8940f350",
            "value": 611445
          }
        },
        "6747f1470898402b81ef8c0a3eb94245": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a4207be31e43ad811c8abd3be7f9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd65765c817a4a35b79cb0d58a4f3a86",
            "placeholder": "​",
            "style": "IPY_MODEL_b431d05102dc4ea28feecc8c4dc13916",
            "value": "model.pt: 100%"
          }
        },
        "b431d05102dc4ea28feecc8c4dc13916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5777468dc0f413e9237218bf5dc9d21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd65765c817a4a35b79cb0d58a4f3a86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6d5a814c6c45ba86470919d259619d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
