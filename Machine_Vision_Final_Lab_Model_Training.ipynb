{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4XrZuAWeoUc",
        "outputId": "26d2227e-7889-4fca-ad2b-11b57957560d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /opt/miniconda3/lib/python3.13/site-packages (4.12.0.88)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.13/site-packages (2.2.6)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/lib/python3.13/site-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n",
        "!pip install opencv-python torch numpy torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMr99Yo7x8N1"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YhoE1nF2Pee"
      },
      "source": [
        "The data for this assignment has been made available and is downloadable to disk by running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ekP01hR9VV",
        "outputId": "2817f7f9-7636-4a38-c171-c2780bb844d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 1_dksksjfwijf.mp4\n",
            "Downloading: 2_dfsaeklnvvalkej.mp4\n",
            "Downloading: 2_difficult_2.mp4\n",
            "Downloading: 2_difficult_sdafkljsalkfj.mp4\n",
            "Downloading: 2_dkdjwkndkfw.mp4\n",
            "Downloading: 2_dkdmkejkeimdh.mp4\n",
            "Downloading: 2_dkjd823kjf.mp4\n",
            "Downloading: 2_dsalkfjalwkenlke.mp4\n",
            "Downloading: 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "Downloading: 2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "Downloading: 2_sadfasjldkfjaseifj.mp4\n",
            "Downloading: 2_sdafkjaslkclaksdjkas.mp4\n",
            "Downloading: 2_sdfkjsaleijflaskdjf.mp4\n",
            "Downloading: 2_sdjfhafsldkjhjk.mp4\n",
            "Downloading: 2_sdkjdsflkjfwa.mp4\n",
            "Downloading: 2_sdlfjlewlkjkj.mp4\n",
            "Downloading: 2_sdlkjsaelijfksdjf.mp4\n",
            "Downloading: 3_asldkfjalwieaskdfaskdf.mp4\n",
            "Downloading: 3_dkk873lkjlksajdf.mp4\n",
            "Downloading: 3_dsjlaeijlksjdfie.mp4\n",
            "Downloading: 3_dsksdfjbvsdkj.mp4\n",
            "Downloading: 3_dslkaldskjflakjs.mp4\n",
            "Downloading: 3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "Downloading: 3_kling_dskfseu.mp4\n",
            "Downloading: 3_kling_kdjflaskdjf.mp4\n",
            "Downloading: 3_sadklfjasbnlkjlfkj.mp4\n",
            "Downloading: 3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "Downloading: 3_sadlfkjawelnflksdjf.mp4\n",
            "Downloading: 3_sdfjwaiejflkasjdf.mp4\n",
            "Downloading: 3_sdflkjliejkjdf.mp4\n",
            "Downloading: 3_sdlkfjaleknaksej.mp4\n",
            "Downloading: 3_sdlkfjalkjejafe.mp4\n",
            "Downloading: 3_sdlkjfaslkjfalskjdf.mp4\n",
            "Downloading: 3_sdlkjslndflkseijlkjef.mp4\n",
            "Downloading: 4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "Downloading: 4_asdlkfjalsflnekj.mp4\n",
            "Downloading: 4_aslkcasckmwlejk.mp4\n",
            "Downloading: 4_aslkjasmcalkewjlkje.mp4\n",
            "Downloading: 4_dssalsdkfjweijf.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "Downloading: 4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "Downloading: 4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "Downloading: 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "Downloading: 4_sadflkjasldkjfalseij.mp4\n",
            "Downloading: 4_sadlfkjlknewkjejk.mp4\n",
            "Downloading: 5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "Downloading: 5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "Downloading: 6_dfjewaijsldkjfsaef.mp4\n",
            "Downloading: 6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "Downloading: 7_sadkjfkljekj.mp4\n",
            "\n",
            "==================================================\n",
            "Downloaded videos:\n",
            "==================================================\n",
            "1_dksksjfwijf.mp4\n",
            "2_dfsaeklnvvalkej.mp4\n",
            "2_difficult_2.mp4\n",
            "2_difficult_sdafkljsalkfj.mp4\n",
            "2_dkdjwkndkfw.mp4\n",
            "2_dkdmkejkeimdh.mp4\n",
            "2_dkjd823kjf.mp4\n",
            "2_dsalkfjalwkenlke.mp4\n",
            "2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "2_sadfasjldkfjaseifj.mp4\n",
            "2_sdafkjaslkclaksdjkas.mp4\n",
            "2_sdfkjsaleijflaskdjf.mp4\n",
            "2_sdjfhafsldkjhjk.mp4\n",
            "2_sdkjdsflkjfwa.mp4\n",
            "2_sdlfjlewlkjkj.mp4\n",
            "2_sdlkjsaelijfksdjf.mp4\n",
            "3_asldkfjalwieaskdfaskdf.mp4\n",
            "3_dkk873lkjlksajdf.mp4\n",
            "3_dsjlaeijlksjdfie.mp4\n",
            "3_dsksdfjbvsdkj.mp4\n",
            "3_dslkaldskjflakjs.mp4\n",
            "3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "3_kling_dskfseu.mp4\n",
            "3_kling_kdjflaskdjf.mp4\n",
            "3_sadklfjasbnlkjlfkj.mp4\n",
            "3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "3_sadlfkjawelnflksdjf.mp4\n",
            "3_sdfjwaiejflkasjdf.mp4\n",
            "3_sdflkjliejkjdf.mp4\n",
            "3_sdlkfjaleknaksej.mp4\n",
            "3_sdlkfjalkjejafe.mp4\n",
            "3_sdlkjfaslkjfalskjdf.mp4\n",
            "3_sdlkjslndflkseijlkjef.mp4\n",
            "4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "4_asdlkfjalsflnekj.mp4\n",
            "4_aslkcasckmwlejk.mp4\n",
            "4_aslkjasmcalkewjlkje.mp4\n",
            "4_dssalsdkfjweijf.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "4_sadflkjasldkjfalseij.mp4\n",
            "4_sadlfkjlknewkjejk.mp4\n",
            "5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "6_dfjewaijsldkjfsaef.mp4\n",
            "6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "7_sadkjfkljekj.mp4\n",
            "\n",
            "Total: 77 files\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import os\n",
        "\n",
        "# Connect to S3 without authentication (public bucket)\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "bucket_name = 'prism-mvta'\n",
        "prefix = 'training-and-validation-data/'\n",
        "download_dir = './video-data'\n",
        "\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# List all objects in the S3 path\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "video_names = []\n",
        "\n",
        "for page in pages:\n",
        "    if 'Contents' not in page:\n",
        "        print(\"No files found at the specified path! Go and complain to the TAs!\")\n",
        "        break\n",
        "\n",
        "    for obj in page['Contents']:\n",
        "        key = obj['Key']\n",
        "        filename = os.path.basename(key)\n",
        "\n",
        "        if not filename:\n",
        "            continue\n",
        "\n",
        "        video_names.append(filename)\n",
        "\n",
        "        local_path = os.path.join(download_dir, filename)\n",
        "        print(f\"Downloading: {filename}\")\n",
        "        s3.download_file(bucket_name, key, local_path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Downloaded videos:\")\n",
        "print(\"=\"*50)\n",
        "for name in video_names:\n",
        "    print(name)\n",
        "\n",
        "print(f\"\\nTotal: {len(video_names)} files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wPAlvHdXj3a"
      },
      "source": [
        "These videos are now available in the folder \"video-data\". You can click on the folder icon on the left-hand-side of this screen to see the videos in a file explorer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtisgbeiYiH_"
      },
      "source": [
        "# Create your Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo9J9hXLeCdY"
      },
      "source": [
        "Some example code for approaching the first *two* TODOs is given below just to get you started. No starter code is given for the third TODO.\n",
        "\n",
        "Note, the below code is very rough skeleton code. Make no assumptions as to the correct manner to architect your model based on the structure of this code.\n",
        "\n",
        "Please feel free to (if not encouraged to) change every single line of the below code (change it to best suit your chosen model architecture, in the next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzDMxGLnYa0s"
      },
      "source": [
        "### TODO 1 (This is mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "Each video in the folder is prefixed by a number. That number corresponds to the number of distinct pushups visible in the video. Write code to iterate over each video in the folder, and extract the corresponding target associated with the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PvwbsYYMlD"
      },
      "source": [
        "### TODO 2 (This is also mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "\n",
        "Divide the data into training and validation sets.\n",
        "\n",
        "Optionally, you can also create out your own test set to assess your performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEaV_5oXZQRc"
      },
      "source": [
        "### TODO 3\n",
        "\n",
        "Any preprocessing or augmentation of your data which you deem required, should (probably) go here. You are also free to include your data-augmentation code later, though doing it before creating your dataloaders is probably a good idea.\n",
        "\n",
        "If you complete this TODO, to maintain experimental hygiene, feel free to modify the code which was provided for TODOs 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvqxH1YBYCUw",
        "outputId": "696c92ae-305c-4588-9296-73e340923a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 62 videos, Val: 15 videos\n",
            "\n",
            "Frames shape: torch.Size([4, 3, 16, 112, 112])\n",
            "Labels: tensor([3, 2, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Here is a basic implementation of the above two TODOs. You can assume the first TODO is completed correctly.\n",
        "\n",
        "# Please modify this code to suit you best, as you decide on your preferred model architecture.\n",
        "\n",
        "# For example, below here we are padding every video to 1,000 frames. That may or may not be a good idea.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.video import R2Plus1D_18_Weights\n",
        "\n",
        "weights = R2Plus1D_18_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "class Augmentation:\n",
        "  \"\"\"\n",
        "  Simple Augmentation for the video; applies flip, brightness, noise and cutout.\n",
        "  \"\"\"\n",
        "  def __init__(self, p=0.3):\n",
        "    self.p = p\n",
        "\n",
        "  def __call__(self, frames):\n",
        "\n",
        "\n",
        "    if random.random() < self.p:\n",
        "      #Horizontal flip\n",
        "      frames = torch.flip(frames, dims=[3]) #flips the W\n",
        "\n",
        "    if random.random() < self.p:\n",
        "      #Random brightness\n",
        "      brightness = 1.0 + random.uniform(-0.2, 0.2)\n",
        "      frames = torch.clamp(frames * brightness, 0, 1)\n",
        "\n",
        "    if random.random() < self.p:\n",
        "      #Add noise\n",
        "      noise = torch.randn_like(frames) * 0.02\n",
        "      frames = torch.clamp(frames + noise, 0, 1)\n",
        "\n",
        "    if random.random() < 0.2:\n",
        "      #Cutout\n",
        "      C, T, H, W = frames.shape\n",
        "      cutout_size = H // 6\n",
        "      x = random.randint(0, H - cutout_size)\n",
        "      y = random.randint(0, W - cutout_size)\n",
        "\n",
        "      frames[:,:, x:x+cutout_size, y:y+cutout_size] = 0\n",
        "\n",
        "    if random.random() < self.p:\n",
        "      #Random Translation\n",
        "      C,T, H, W = frames.shape\n",
        "      max_shift = 8 #num of pixels\n",
        "      dx = random.randint(-max_shift, max_shift)\n",
        "      dy = random.randint(-max_shift, max_shift)\n",
        "\n",
        "      #pad and crop to the original size\n",
        "      frames = F.pad(frames, (max_shift, max_shift, max_shift, max_shift))\n",
        "      frames = frames[:, :, (max_shift+dy):(max_shift+dy+H), (max_shift+dx):(max_shift+dx+W)]\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"Dataset for loading videos from a folder. Labels from filename prefix.\"\"\"\n",
        "\n",
        "    def __init__(self, video_dir, frame_size=(112, 112), target_frames=16, augment=None, do_preprocess=True):\n",
        "        self.video_dir = video_dir\n",
        "        self.frame_size = frame_size\n",
        "        self.target_frames = target_frames\n",
        "        self.augment = augment\n",
        "        self.do_preprocess = do_preprocess\n",
        "\n",
        "        self.video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "        self.labels = [int(f.split('_')[0]) - 1 for f in self.video_files] \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
        "        frames = self._load_video(video_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            frames = self.augment(frames)\n",
        "        \n",
        "        if self.do_preprocess:\n",
        "           x = frames.permute(1,0,2,3)\n",
        "           x = preprocess(x)\n",
        "           frames = x.permute(1,0,2,3)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def _load_video(self, path, target_frames=16):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        all_frames = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            all_frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        T = self.target_frames\n",
        "        H, W = self.frame_size\n",
        "\n",
        "        if len(all_frames) == 0:\n",
        "            return torch.zeros(3, T, H, W)\n",
        "\n",
        "        idxs = np.linspace(0, len(all_frames) - 1, T).astype(int)\n",
        "        sampled = [cv2.resize(all_frames[i], (W, H)) for i in idxs]\n",
        "\n",
        "        frames = torch.from_numpy(np.array(sampled)).permute(3, 0, 1, 2).float() / 255.0\n",
        "\n",
        "        return frames\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    frames_list, labels = zip(*batch)\n",
        "    frames = torch.stack(frames_list)\n",
        "    frames = frames.permute(0, 2, 1, 3, 4)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return frames, labels\n",
        "\n",
        "\n",
        "def get_dataloaders(video_dir, batch_size=4, val_split=0.2, frame_size=(112, 112), target_frames=16):\n",
        "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
        "\n",
        "    files = sorted([f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
        "\n",
        "    train_dataset_full = VideoDataset(\n",
        "        video_dir,\n",
        "        frame_size=frame_size,\n",
        "        target_frames=target_frames,\n",
        "        augment=Augmentation(p=0.5),\n",
        "        do_preprocess=True\n",
        "    )\n",
        "    train_dataset_full.video_files = files\n",
        "    train_dataset_full.labels = [int(f.split('_')[0]) - 1 for f in files]\n",
        "\n",
        "    val_dataset_full = VideoDataset(\n",
        "        video_dir,\n",
        "        frame_size=frame_size,\n",
        "        target_frames=target_frames,\n",
        "        augment=None,\n",
        "        do_preprocess=True\n",
        "    )\n",
        "    val_dataset_full.video_files = files\n",
        "    val_dataset_full.labels = [int(f.split('_')[0]) - 1 for f in files]\n",
        "\n",
        "    val_size = int(len(files) * val_split)\n",
        "    train_size = len(files) - val_size\n",
        "\n",
        "    gen = torch.Generator().manual_seed(42)\n",
        "    train_idx, val_idx = random_split(range(len(files)), [train_size, val_size], generator=gen)\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset_full, train_idx.indices)\n",
        "    val_dataset   = torch.utils.data.Subset(val_dataset_full, val_idx.indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)} videos, Val: {len(val_dataset)} videos\\n\")\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "video_dir = './video-data'\n",
        "\n",
        "train_loader, val_loader = get_dataloaders(video_dir, batch_size=4, val_split=0.2)\n",
        "\n",
        "for frames, labels in train_loader:\n",
        "    print(f\"Frames shape: {frames.shape}\")  # (B, C, 1000, H, W)\n",
        "    print(f\"Labels: {labels}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPYRadrZdty"
      },
      "source": [
        "# Create a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrpSDGMWaBR3"
      },
      "source": [
        "For this assignment, we request you use PyTorch. Below is an example of how to instantiate a very basic PyTorch model.\n",
        "\n",
        "Note, this model below needs a _lot_ of work.\n",
        "\n",
        "Please include your code for creating your model below.\n",
        "\n",
        "The only constraint here is that you define a Python object which inherits from a PyTorch nn.Module object. Beyond that, please feel free to implement anything you like: Transformer, Vision Transformer, MLP, CNN, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRolEQeAbxsx"
      },
      "source": [
        "### TODO 4\n",
        "\n",
        "Create your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgcBSMi6fu75",
        "outputId": "7f6413e2-c35b-4a8a-c403-383afc5f7fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_classes: 7\n"
          ]
        }
      ],
      "source": [
        "labels = []\n",
        "for f in os.listdir(\"./video-data\"):\n",
        "    if f.endswith(\".mp4\"):\n",
        "        labels.append(int(f.split(\"_\")[0]) - 1)\n",
        "print(\"num_classes:\", len(set(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5FlYz3paNxu",
        "outputId": "d65e3889-6053-4e80-93ae-e7f81e7be6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=512, out_features=7, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "#Downloads the pretrained  model\n",
        "num_classes = 7\n",
        "weights = R2Plus1D_18_Weights.DEFAULT\n",
        "model = r2plus1d_18(weights=weights)\n",
        "\n",
        "#Swaps the last layer so it outputs 8 logits\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.layer4.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "print(model.fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipi6oAFGcvkj",
        "outputId": "91598885-26c7-429a-eed0-53b54bbb8601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 16, 112, 112])\n",
            "torch.Size([4, 7])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "frames, labels = next(iter(train_loader))\n",
        "frames = frames.to(device)\n",
        "out = model(frames)\n",
        "\n",
        "print(frames.shape)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bou97f8czAu"
      },
      "source": [
        "# Train your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSqu_ZkcFxs"
      },
      "source": [
        "### TODO 5\n",
        "\n",
        "Training time! Please include your training code below.\n",
        "\n",
        "As per above, please feel free (and encouraged) to rip out all of the below code and replace with your (much better) code.\n",
        "\n",
        "The below should just be used as an example to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EueH4HSdcLlE"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for frames, labels in train_loader:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_loader:\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs, lr, weight_decay):\n",
        "    device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHI68u4XhCGB",
        "outputId": "1b8c2c3b-a2d1-444d-96a5-e494a1d491cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n",
            "Epoch 1/8 | Train Loss: 2.3855, Train Acc: 0.0484 | Val Loss: 2.2096, Val Acc: 0.1333\n"
          ]
        }
      ],
      "source": [
        "# setting a manual seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "model = train_model(model, train_loader, val_loader, epochs=8, lr=1e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gmJS-yn2qc"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzS5ADbDn6Yr"
      },
      "source": [
        "## TODO 6\n",
        "\n",
        "Include any code which you feel is useful for evaluating your model performance below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1KwRou4oCkj"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate(model, loader, device, num_classes=7):\n",
        "  model.eval()\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for frames, labels in loader:\n",
        "      frames = frames.to(device)\n",
        "      labels = labels.to(device)\n",
        "      logits = model(frames)\n",
        "      preds = logits.argmax(dim=1)\n",
        "      all_preds.append(preds.cpu().numpy())\n",
        "      all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "  y_pred = np.concatenate(all_preds)\n",
        "  y_true = np.concatenate(all_labels)\n",
        "\n",
        "  acc = (y_pred == y_true).mean()\n",
        "  print(f\"Val accuracy: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5qRr1wJIkvW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val accuracy: 0.533\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAmXb-QC2ChR"
      },
      "source": [
        "# Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl3rU9Ec4uSI"
      },
      "source": [
        "It is a requirement of this assignment that you submit your trained model to a repo on Hugging Face, and make it publicly available. Below, we provide code which should help you do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSUcEj-DoI8K"
      },
      "source": [
        "## TODO 7\n",
        "\n",
        "Upload your model to HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtUkkHpCtQaB"
      },
      "source": [
        "Install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lYdo05DftcBC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /opt/miniconda3/lib/python3.13/site-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.13/site-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIlD5u1U5IBo"
      },
      "source": [
        "You'll now need to log in to Hugging Face via the command line. To do this, you'll need to generate a token on your Hugging Face account. To generate a token, run the below command, and click on the link which appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y9d3loOxtf7v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): ^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/bin/hf\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/cli/hf.py\", line 59, in main\n",
            "    service.run()\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/cli/auth.py\", line 126, in run\n",
            "    login(\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/_login.py\", line 124, in login\n",
            "    interpreter_login(new_session=new_session)\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/opt/miniconda3/lib/python3.13/site-packages/huggingface_hub/_login.py\", line 281, in interpreter_login\n",
            "    token = getpass(\"Enter your token (input will not be visible): \")\n",
            "  File \"/opt/miniconda3/lib/python3.13/getpass.py\", line 76, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "  File \"/opt/miniconda3/lib/python3.13/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEnw3O5I5kY8"
      },
      "source": [
        "The below code will only run if you have already trained a model with variable name 'model'.\n",
        "\n",
        "The below code will take your trained model, and upload it to a *public* HuggingFace repo in your account called \"mv-final-assignment\".\n",
        "\n",
        "(Note - in this example, we have set 'private=False' in the upload_to_hub method. This makes your model public).\n",
        "\n",
        "You should double-check that your model is in fact public. To do that, you can navigate (in an incognito tab, in a browser) to https://huggingface.co/YOUR_USERNAME/YOUR_MODEL_NAME and see if that page loads. If your model is public, it will. (Simply being able to run the below code will not guarantee that your model is in fact public, because, you have now authenticated yourself with the huggingface CLI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCq_stTaoeQW"
      },
      "outputs": [],
      "source": [
        "# YOUR HUGGING FACE USERNAME BELOW\n",
        "hf_username = 'EleftheriaK'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AdQof5XtWfS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "\n",
        "\n",
        "def save_model(model, path=\"model.pt\"):\n",
        "    \"\"\"Save the model weights to a file.\"\"\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "\n",
        "def upload_to_hub(local_path=\"model.pt\", repo_id=f\"{hf_username}/mv-final-assignment\"):\n",
        "    \"\"\"\n",
        "    Upload model to Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        local_path: Path to your saved model file\n",
        "        repo_id: Your repo in format \"username/model-name\"\n",
        "    \"\"\"\n",
        "    api = HfApi()\n",
        "\n",
        "    # Create the repo first (if it already exists, this will just skip)\n",
        "    api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        exist_ok=True,  # Don't error if it already exists\n",
        "        private=False,  # Make it public so TAs can access\n",
        "    )\n",
        "\n",
        "    # Now upload the file\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=local_path,\n",
        "        path_in_repo=\"model.pt\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "\n",
        "    print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    save_model(model, \"mv-final-assignment.pt\")\n",
        "\n",
        "    upload_to_hub(\"mv-final-assignment.pt\", f\"{hf_username}/mv-final-assignment\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
